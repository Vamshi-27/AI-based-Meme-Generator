import torch
from transformers import GPT2Tokenizer, GPT2LMHeadModel
import clip
from PIL import Image
import os
from torch.utils.data import DataLoader
from src.data_preparation import MemeDataset

class MemeCaptionModel:
    def __init__(self, device="cuda" if torch.cuda.is_available() else "cpu"):
        self.device = torch.device(device)
        
        # Load models
        self.clip_model, self.clip_preprocess = clip.load("ViT-B/32", device=self.device)
        self.gpt2_tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
        self.gpt2_model = GPT2LMHeadModel.from_pretrained("gpt2").to(self.device)

    def get_image_features(self, image):
        """
        Extract image features using CLIP.
        """
        image_input = self.clip_preprocess(image).unsqueeze(0).to(self.device)
        with torch.no_grad():
            image_features = self.clip_model.encode_image(image_input)
        return image_features

    def train(self, data_loader, num_epochs=5):
        """
        Training logic to fine-tune GPT-2 on captions generated by CLIP.
        """
        optimizer = torch.optim.Adam(self.gpt2_model.parameters(), lr=0.001)

        for epoch in range(num_epochs):
            total_loss = 0

            for image_path, caption in data_loader:
                image = Image.open(image_path).convert("RGB")
                image_features = self.get_image_features(image)

                # Prepare the input prompt for GPT-2
                text_input = f"Image features: {image_features.sum().item()}. Caption: "
                input_ids = self.gpt2_tokenizer.encode(text_input, return_tensors="pt").to(self.device)
                labels = self.gpt2_tokenizer.encode(caption, return_tensors="pt").to(self.device)

                outputs = self.gpt2_model(input_ids, labels=labels)
                loss = outputs.loss
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

                total_loss += loss.item()

            print(f"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(data_loader)}")

    def save_model(self, path):
        torch.save(self.gpt2_model.state_dict(), path)

